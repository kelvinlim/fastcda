================================================================================
Scores
================================================================================
bdeu-score
--------------------------------------------------------------------------------
This is the BDeu score given in Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3), 197-243. This gives a score for any two variables conditioned on any list of others which is more positive for distributions which are more strongly dependent.


cg-bic-score
--------------------------------------------------------------------------------
Conditional Gaussian BIC Score may be used for the case where there is a mixture of discrete and Gaussian variables. Calculates a BIC score based on likelihood that is calculated using a conditional Gaussian assumption. See Andrews, B., Ramsey, J., & Cooper, G. F. (2018). Scoring Bayesian networks of mixed variables. International journal of data science and analytics, 6(1), 3-18. Note that BIC is calculated as 2L - k ln N, so "higher is better."


dg-bic-score
--------------------------------------------------------------------------------
Degenerate Gaussian BIC Score may be used for the case where there is a mixture of discrete and Gaussian variables. Calculates a BIC score based on likelihood that is calculated using a conditional Gaussian assumption. See Andrews, B., Ramsey, J., & Cooper, G. F. (2019). Learning high-dimensional directed acyclic graphs with mixed data-types. Proceedings of machine learning research, 104, 4. Note that BIC is calculated as 2L - k ln N, so "higher is better."


disc-bic-score
--------------------------------------------------------------------------------
This is a BIC score for the discrete case. The likelihood is judged by the multinomial tables directly, and this is penalized as is usual for a BIC score. The only surprising thing perhaps is that we use the formula BIC = 2L - k ln N, where L is the likelihood, k the number of parameters, and N the sample size, instead of the usual L + k / 2 ln N. So higher BIC scores will correspond to greater dependence. In the case of independence, the BIC score will be negative, since the likelihood will be zero, and this will be penalized.


ebic-score
--------------------------------------------------------------------------------
This is the Extended BIC (EBIC) score of Chen and Chen (Chen, J., & Chen, Z. (2008). Extended Bayesian information criteria for model selection with large model spaces. Biometrika, 95(3), 759-771.). This score is adapted to score-based search in high dimensions. There is one parameter, gamma, which takes a value between 0 and 1; if it's 0, the score is standard BIC. A value of 0.5 or 1 is recommended depending on how many variables there are per sample.


gic-scores
--------------------------------------------------------------------------------
This is a set of generalized information criterion (GIC) scores based on the paper, Kim, Y., Kwon, S., & Choi, H. (2012). Consistent model selection criteria on high dimensions. The Journal 0of Machine Learning Research, 13(1), 1037-1057. One needs to select which lambda to use in place of the usual lambda for the linear, Gaussian BIC score. A penalty discount parameter may also be specified, though this is by default for these scores equal to 1 (since the lambda choice is essentially picking a penalty discount for you).


m-sep-score
--------------------------------------------------------------------------------
This uses m-separation to make something that acts as a score if you know the true graph. A score in Tetrad, for FGES, say, is a function that for X and Y conditional on Z, returns a negative number if X _||_ Y | Z and a positive number otherwise. So to get this behavior in no u certain terms, we simply return -1 for independent cases and +1 for dependent cases. Works like a charm. This can be used for FGES to check what the ideal behavior of the algorithm should be. Simply draw an edge from the true graph to the search box, select FGES, and search as usual.


poisson-prior-score
--------------------------------------------------------------------------------
This is likelihood score attenuated by the log of the Poisson distribution. It has one parameter, lambda, from the Poisson distribution, which acts as a structure prior.


sem-bic-score
--------------------------------------------------------------------------------
This is specifically a BIC score for the linear, Gaussian case, where we include an additional penalty term, which is commonly used. We call this the penalty discount. So our formulas has BIC = 2L - ck log N, where L is the likelihood, c the penalty discount (usually greater than or equal to 1), and N the sample size. Since the assumption is that the data are distributed as Gaussian, this reduces to BIC = -n log sigma - ck ln N, where sigma is the standard deviation of the linear residual obtained by regressing a child variable onto all of its parents in the model.


zsbound-score
--------------------------------------------------------------------------------
Uses Theorem 1 from Zhang, Y., & Shen, X. (2010). Model selection procedure for high‚Äêdimensional data. Statistical Analysis and Data Mining: The ASA Data Science Journal, 3(5), 350-358, to make a score that controls false positives. The is one parameter, the "risk bound", a number between 0 and 1 (a bound on false positive risk probability).


